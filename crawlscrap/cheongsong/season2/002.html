<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 25px;
 
 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 80px;
    
    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;
    
    line-height:1.6em
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">    
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"], 
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                   displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>
002. crawling comments
<xmp>
@
import requests
from bs4 import BeautifulSoup
from urllib import parse
import nltk
from konlpy.tag import Twitter
from matplotlib import font_manager, rc
font_name = font_manager.FontProperties(fname="/Library/Fonts/AppleGothic.ttf").get_name()
rc('font', family=font_name)
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# You can find "more" element by using record button in "network" tab
def get_comments(news_link, comment_count, comments):
    # I store comments in list "comment_list"
    comment_list = []
    # I can see headers in request information
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36',
        'referer' : 'http://news.naver.com/main/read.nhn?m_view=1&includeAllCount=true&mode=LSD&mid=sec&sid1=100&oid=001&aid=0009804092'
    }

    # http://news.naver.com/main/read.nhn?m_view=1&includeAllCount=true&mode=LSD&mid=sec&sid1=100&oid=001&aid=0009804092

    aid = news_link.split("aid=")[1]
    oid = news_link.split("oid=")[1].split("&")[0]

    page = 1

    while True:

        r = requests.get("https://apis.naver.com/commentBox/cbox/web_neo_list_jsonp.json?ticket=news&templateId=default_politics&pool=cbox5&_callback=jQuery1709499279192395464_1515568397621&lang=ko&country=&objectId=news" + oid + "%2C" + aid + "&categoryId=&pageSize=20&indexSize=10&groupId=&listType=OBJECT&pageType=more&page=" + str(page) + "&refresh=false&sort=FAVORITE&current=1163096813&prev=1163083753&includeAllStatus=true&_=1515568468166", headers=headers)
        c = r.content
        soup = BeautifulSoup(c,"html.parser")
        print(soup)

        c_count = int(str(soup).split('comment":')[1].split(",")[0])
        contents = str(soup).split('contents":"')

        for i in range(1, len(contents)):
            comment_content = contents[i].split('","userIdNo":')[0]
            comment_list.append(comment_content)
            comment_count = comment_count + 1

            if int(comment_count) == int(comments):
                return comment_list, True

        if c_count < 1 or c_count == page:
            return comment_list, False
        else:
            page = page + 1


def keyword_search(keyword, page_count):
    naver_news_links = []

    i=0
    j=1

    # It's recommended to send header when you request to server
    # Without this, you can be banned
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'
    }

    while True:
        # from urllib import parse
        # I encode keyword (which user input) into url type by using parse.quote()
        # str(j) means the number of news articles
        r = requests.get('https://search.naver.com/search.naver?ie=utf8&where=news&query=' + parse.quote(keyword) + '&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=0&ds=&de=&docid=&nso=so:r,p:all,a:all&mynews=0&cluster_rank=19&start=' + str(j) + '&refresh_start=0',headers=headers)
        
        c = r.content
        soup = BeautifulSoup(c, "html.parser")

        news_list = soup.find_all('a',{"class":"_sp_each_url"})
        for n in news_list:
            if n.text == "네이버뉴스":
                naver_news_links.append(n['href'])
                i = i + 1
                if i == int(page_count):
                    return naver_news_links

        j = j + 10


# I define __main__()
if __name__ == '__main__':
    # First, I will take "keywords" from user
    keyword = input("$ Input keywords : ")
    news = input("$ Input the number of news to be crawled : ")
    comments = input("$ Input the number of comments to be crawled : ")

    news_links = keyword_search(keyword, news)
    print(news_links)
    print(len(news_links))

    # The accumulated number of crawled comments
    # 10 comments from news1
    # 30 comments from news2
    # comment_count will be 40
    comment_count = 0
    comment_list = []

    for news_link in news_links:
        #l, flag = get_comments(news_link, comment_count, comments)
        get_comments(news_link, comment_count, comments)
    #     comment_list.extend(l)
    #     comment_count = int(comment_count + len(l))
    #
    #     if flag is True:
    #         break
    #
    # analzye(comment_list, keyword) 
      </xmp>
   </BODY>
</HTML>
